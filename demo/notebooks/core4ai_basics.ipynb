{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Core4AI in Jupyter Notebooks\n",
    "\n",
    "Core4AI (Contextual Optimization and Refinement Engine for AI) is a powerful tool that transforms basic user queries into optimized prompts for AI systems using MLflow Prompt Registry. In this guide, we'll explore how to set up and use Core4AI in a Jupyter Notebook environment, covering everything from initial configuration to practical applications.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Environment Setup](#environment-setup)\n",
    "2. [Initial Configuration](#initial-configuration)\n",
    "3. [Working with Prompt Templates](#working-with-prompt-templates)\n",
    "4. [Switching Between AI Providers](#switching-between-ai-providers)\n",
    "5. [Generating Enhanced Responses](#generating-enhanced-responses)\n",
    "6. [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, let's set up our environment by installing Core4AI and its dependencies. In a notebook cell, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Core4AI\n",
    "!pip install core4ai==1.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from core4ai import Core4AI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Configuration\n",
    "\n",
    "### Connecting to MLflow Server\n",
    "\n",
    "Core4AI uses MLflow to store and manage prompt templates. Let's configure it to connect to a local MLflow server running on port 8080:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Core4AI instance\n",
    "ai = Core4AI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure MLflow\n",
    "ai.set_mlflow_uri(\"http://localhost:8080\")\n",
    "\n",
    "# Verify configuration\n",
    "ai.get_current_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up OpenAI Provider\n",
    "\n",
    "To use OpenAI as the default provider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI (using environment variable for API key)\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "ai.configure_openai(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Save configuration\n",
    "ai.save_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Ollama Provider\n",
    "\n",
    "Alternatively, if you're using Ollama (local LLM server):\n",
    "\n",
    "```python\n",
    "# Set up Ollama\n",
    "ai.configure_ollama(uri=\"http://localhost:11434\", model=\"llama3.2:latest\")\n",
    "\n",
    "# Save configuration\n",
    "ai.save_config()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Prompt Templates\n",
    "\n",
    "### Registering Sample Prompts\n",
    "\n",
    "Core4AI comes with pre-defined prompt templates. Let's register them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register sample prompts\n",
    "result = ai.register_samples()\n",
    "print(f\"Registered {result.get('registered', 0)} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing Available Prompts\n",
    "\n",
    "To see what prompts are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available prompts\n",
    "prompts = ai.list_prompts()\n",
    "\n",
    "# Display prompt names and types\n",
    "if prompts.get(\"status\") == \"success\":\n",
    "    prompt_list = prompts.get(\"prompts\", [])\n",
    "    for p in prompt_list:\n",
    "        print(f\"{p['name']} ({p['type']}) - Variables: {', '.join(p['variables'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Custom Prompt Templates\n",
    "\n",
    "You can create your own prompt templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new prompt template\n",
    "template_result = ai.create_prompt_template(\"data_visualization_prompt\", \n",
    "                                           output_dir=\"./my_prompts\")\n",
    "\n",
    "# The template file is created at the specified location\n",
    "# Edit it manually, then register it\n",
    "if template_result.get(\"status\") == \"success\":\n",
    "    file_path = template_result.get(\"file_path\")\n",
    "    print(f\"Edit the template at: {file_path} and then register it\")\n",
    "    \n",
    "    # After editing, register the template\n",
    "    ai.register_from_markdown(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Registration\n",
    "\n",
    "You can also register prompts directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a prompt directly\n",
    "ai.register_prompt(\n",
    "    name=\"quick_analysis_prompt\",\n",
    "    template=\"Analyze the following {{ data_type }} data and provide {{ analysis_type }} insights:\\n\\n```\\n{{ data }}\\n```\\n\\nFocus on {{ focus_area }} in your analysis.\",\n",
    "    tags={\"type\": \"quick_analysis\", \"task\": \"data_analysis\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switching Between AI Providers\n",
    "\n",
    "Core4AI allows easy switching between providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to OpenAI\n",
    "def use_openai():\n",
    "    ai.configure_openai(model=\"gpt-4o\")\n",
    "    ai.save_config()\n",
    "    print(\"Now using OpenAI with model: gpt-4o\")\n",
    "    \n",
    "# Switch to Ollama\n",
    "def use_ollama():\n",
    "    ai.configure_ollama(uri=\"http://localhost:11434\", model=\"llama3.2:latest\")\n",
    "    ai.save_config()\n",
    "    print(\"Now using Ollama with model: llama3.2:latest\")\n",
    "\n",
    "# Use OpenAI for this example\n",
    "use_openai()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Enhanced Responses\n",
    "\n",
    "Now let's use Core4AI to generate enhanced responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple query\n",
    "response = ai.chat(\"Write about the benefits of machine learning\")\n",
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Getting Enhanced Query Details\n",
    "\n",
    "To see how Core4AI enhances queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verbose output to see the enhancement process\n",
    "response = ai.chat(\"Write an email to my boss about a project delay\", verbose=True)\n",
    "\n",
    "print(\"Original Query:\", response[\"original_query\"])\n",
    "print(\"\\nMatched to:\", response[\"prompt_match\"].get(\"prompt_name\", \"No match\"))\n",
    "print(\"Confidence:\", response[\"prompt_match\"].get(\"confidence\", 0), \"%\")\n",
    "print(\"\\nEnhanced Query:\", response[\"enhanced_query\"])\n",
    "print(\"\\nResponse:\", response[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Specific Prompt Types\n",
    "\n",
    "To use a specific prompt type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Craft a query that clearly matches a technical prompt\n",
    "response = ai.chat(\"Explain how neural networks work to a beginner\", verbose=True)\n",
    "print(f\"Matched to: {response['prompt_match'].get('prompt_name')}\")\n",
    "print(f\"Content type: {response['content_type']}\")\n",
    "print(\"\\nResponse:\", response[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Type Detection\n",
    "\n",
    "Core4AI automatically detects content types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test various query types to see content detection\n",
    "test_queries = [\n",
    "    \"Write an essay about climate change\",\n",
    "    \"Draft an email to a client about project progress\",\n",
    "    \"Create a Python function to sort a list\",\n",
    "    \"Compare electric cars and gasoline cars\",\n",
    "    \"Summarize the key points about quantum computing\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    response = ai.chat(query)\n",
    "    content_type = response.get(\"content_type\", \"Unknown\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"Detected content type: {content_type}\")\n",
    "    print(f\"Matched prompt: {response['prompt_match'].get('prompt_name', 'None')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This guide has demonstrated how to:\n",
    "- Configure Core4AI in a Jupyter Notebook\n",
    "- Register and manage prompt templates\n",
    "- Switch between AI providers\n",
    "- Generate enhanced responses\n",
    "\n",
    "Core4AI provides a powerful abstraction layer that helps you create more consistent, high-quality AI responses by automatically matching user queries to appropriate prompt templates and optimizing them before sending to the LLM provider.\n",
    "\n",
    "The ability to switch seamlessly between cloud-based providers like OpenAI and local models via Ollama gives you flexibility to choose the right solution for your specific needs, whether you need top-tier capabilities or prefer local privacy-focused deployment.\n",
    "\n",
    "Happy prompting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "privateGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
